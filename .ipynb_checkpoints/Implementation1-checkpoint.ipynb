{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read CSVs\n",
    "def get_data():\n",
    "    \"\"\"\n",
    "    Get training, validation and test data\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(\"./PA1_train.csv\")\n",
    "    dev = pd.read_csv(\"./PA1_dev.csv\")\n",
    "    test = pd.read_csv(\"./PA1_test.csv\")\n",
    "    return train,dev,test\n",
    "\n",
    "#Add bias\n",
    "def add_bias(df):\n",
    "    \"\"\"\n",
    "    Add dummy variable to control intercept\n",
    "    \"\"\"\n",
    "    df[\"dummy\"] = 1\n",
    "    return df\n",
    "\n",
    "#Seperate Features from response\n",
    "def seperate(df_train):\n",
    "    \"\"\"\n",
    "    input: dataframe\n",
    "    \"\"\"\n",
    "    #Grab all continuous features\n",
    "    x = df_train.iloc[:,0:-1]\n",
    "    #Split dates\n",
    "    x = split_date(x)\n",
    "    x = x.drop([\"date\"], axis=1)\n",
    "    #Grab response y\n",
    "    y = df_train.iloc[:,-1]\n",
    "    \n",
    "    return x,y\n",
    "    \n",
    "    \n",
    "#Add new features [Month, Day, Year]\n",
    "def split_date(df_train):\n",
    "    \"\"\"\n",
    "    splits date into seperate features\n",
    "    input: dataframe\n",
    "    \"\"\"\n",
    "    print(\"Splitting date...\")\n",
    "    for i in range(0,len(df_train)):\n",
    "        df_train.loc[i,\"month\"] = int(df_train.loc[i,\"date\"].split(\"/\")[0])\n",
    "        df_train.loc[i,\"day\"] = int(df_train.loc[i,\"date\"].split(\"/\")[1])\n",
    "        df_train.loc[i,\"year\"] = int(df_train.loc[i,\"date\"].split(\"/\")[2])\n",
    "    print(\"Done\")\n",
    "    return df_train\n",
    "\n",
    "#Normalize data\n",
    "def normalize(df1):\n",
    "    \"\"\"\n",
    "    Normalizes feature matrix\n",
    "    input: feature df\n",
    "    \"\"\"\n",
    "    print(\"Normalizing...\")\n",
    "    x = (df1 - np.min(df1))/(np.max(df1) - np.min(df1))\n",
    "    print(\"DONE\")\n",
    "    return x\n",
    "\n",
    "#Linear regression function\n",
    "def linear_regress(x,y,eta,t,lamb):\n",
    "    \"\"\"\n",
    "    x: input/features\n",
    "    y: opuput\n",
    "    eta: learning rate\n",
    "    t: iterations\n",
    "    lamb: regularization constant\n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    e = np.zeros(len(y))\n",
    "    errors = []\n",
    "    gradient = []\n",
    "    \n",
    "    #Initialize weights [w] and predictions [y_hat]\n",
    "    w = np.zeros(len(x[0]))\n",
    "    \n",
    "     \n",
    "    while n<t:\n",
    "        #Initialize gradient for each epoch\n",
    "        gradient_vector = np.zeros(len(x[0]))\n",
    "                \n",
    "        #Traverse through each data point   \n",
    "        for i in range(len(x)):\n",
    "            #Predicted value\n",
    "            y_hat = np.dot(w.T,x[i])\n",
    "\n",
    "            #Error\n",
    "            e[i] = ((y[i] - y_hat)**2)\n",
    "            \n",
    "            \n",
    "            #Regularization\n",
    "            if np.dot(w.T,w) == 0:\n",
    "                r = 0\n",
    "            else:\n",
    "                r = (np.dot(w.T,w))**0.5\n",
    "            \n",
    "            #Traverse through each feature to update corresponding weights\n",
    "            for j in range(len(x[0])):\n",
    "                gradient_vector[j] += ((-2)*(y[i] - y_hat)*x[i,j]) + (2*lamb*r)\n",
    "            \n",
    "        #Update weights\n",
    "        w -=  eta*gradient_vector\n",
    "        #Calculate SSE\n",
    "        errors.append(sum(e))\n",
    "        #Norm of gradient\n",
    "        gradient.append(np.dot(gradient_vector.T,gradient_vector)**0.5)\n",
    "        \n",
    "        ####\n",
    "        print(f'#####Iteration : {n+1}#####')\n",
    "        print(f'Gradient : {gradient[n]}')\n",
    "        \n",
    "        ####\n",
    "        \n",
    "        n += 1\n",
    "        if (np.dot(gradient_vector.T,gradient_vector)**0.5) < 0.5:\n",
    "            return w,errors,gradient\n",
    "    \n",
    "    return w,errors,gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting date...\n",
      "Done\n",
      "Normalizing...\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#####DATA PREP#####\n",
    "\n",
    "#load csv\n",
    "df_train,df_dev,df_test = get_data()\n",
    "#Drop ID Feature\n",
    "df_train = df_train.drop(\"id\", axis=1)\n",
    "\n",
    "#Grab features and Response\n",
    "x,y = seperate(df_train)\n",
    "\n",
    "#Normalize continuous features\n",
    "x_norm_df = normalize(x)\n",
    "#Add Bias \n",
    "x_norm_df = add_bias(x_norm_df)\n",
    "x_norm = x_norm_df.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Iteration : 1#####\n",
      "Gradient : 205280.8150291437\n",
      "#####Iteration : 2#####\n",
      "Gradient : 140488987.2017579\n",
      "#####Iteration : 3#####\n",
      "Gradient : 96308260401.9164\n",
      "#####Iteration : 4#####\n",
      "Gradient : 66021444183852.414\n",
      "#####Iteration : 5#####\n",
      "Gradient : 4.525916131948607e+16\n",
      "#####Iteration : 6#####\n",
      "Gradient : 3.1026156859595555e+19\n",
      "#####Iteration : 7#####\n",
      "Gradient : 2.1269117266249084e+22\n",
      "#####Iteration : 8#####\n",
      "Gradient : 1.4580450660796138e+25\n",
      "#####Iteration : 9#####\n",
      "Gradient : 9.995221654509298e+27\n",
      "#####Iteration : 10#####\n",
      "Gradient : 6.851945680348161e+30\n"
     ]
    }
   ],
   "source": [
    "weights,sse,gradient = linear_regress(x_norm, y, 0.01, 10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[417729.13677477534,\n",
       " 143754355571.63043,\n",
       " 6.755267439128267e+16,\n",
       " 3.1745743328271015e+22,\n",
       " 1.4918613554582054e+28,\n",
       " 7.01086215235739e+33,\n",
       " 3.294688741653647e+39,\n",
       " 1.5483079924384758e+45,\n",
       " 7.2761278148710415e+50,\n",
       " 3.419347845318549e+56]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
